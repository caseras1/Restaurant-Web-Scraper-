{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efce1f18-83e6-43dd-9394-5c7de6ffdfa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Brasserie de Canette → 4.4\n",
      "✅ Cafe De Lange Heer → 4.5\n",
      "✅ Blend Haarlem → 4.2\n",
      "✅ The Governor → 4.3\n",
      "✅ Restaurant Éclusier → 5\n",
      "✅ Southern Cross → 4.7\n",
      "✅ Brasserie van Beinum → 4.2\n",
      "✅ Restaurant Rood → 4.7\n",
      "✅ Guus Koffie → 4.8\n",
      "✅ Toko SamaSama → 4.5\n",
      "✅ FLFL → 4.8\n",
      "✅ By LIMA → 4.5\n",
      "✅ De slagersdochter → 4.5\n",
      "✅ Friethoes (De Winkel) → 4.7\n",
      "✅ Rigatoni → 4.6\n",
      "✅ Adamo → 4.6\n",
      "✅ Mano → 4.9\n",
      "✅ Club Cantina → 4.6\n",
      "✅ Kraantje Lek → 4.2\n",
      "✅ Nolita → 4.4\n",
      "✅ Restaurant Locael Centraal → 4.2\n",
      "✅ Restaurant Locael Bloemendaal → 3.9\n",
      "✅ Teds Haarlem → 4.2\n",
      "✅ Menu Corridor → 4.4\n",
      "✅ Café Colette → 4.4\n",
      "✅ Restaurant Metzo → 4.5\n",
      "✅ Museumcafé Thuys → 4.5\n",
      "✅ Kus van de Cactus → 4.5\n",
      "✅ The Harlem Social Club → 4.7\n",
      "✅ Frenchie Restaurant → 4.5\n",
      "✅ Restaurant Fris → 4.6\n",
      "✅ Friethuis La Petite → 4.7\n",
      "✅ Bambu Kitchen & Bar → 4.3\n",
      "✅ Republiek Bloemendaal → 3.8\n",
      "✅ Five Brothers Fat → 4.6\n",
      "✅ Bistrobar Indonesia → 4.3\n",
      "✅ Maita → 4.4\n",
      "✅ Olivers Haarlem → 4.7\n",
      "✅ Houtbaar → 4.9\n",
      "✅ kaldi → 4.7\n",
      "✅ Pip Deli → 4.8\n",
      "✅ Red Orchids → 4.5\n",
      "✅ Meneer Fans → 4.4\n",
      "✅ Mooie Boules → 4.3\n",
      "✅ The Louisiana Lobstershack → 4.3\n",
      "✅ Monsier Rouge → 4.6\n",
      "✅ De VolksLust → 4\n",
      "✅ Bar Taru → 4.7\n",
      "✅ Ratatouille Food & Wine → 4.6\n",
      "✅ Toast → 4.8\n",
      "✅ Your Company on Haarlem Today → 4.3\n",
      "✅ Saved to 'haarlem_today_with_full_google_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = \"AIzaSyBPXacrTdwVOZ8vS6nlywLkD3D49Yipz2Q\"\n",
    "SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "\n",
    "# Load your own restaurant names\n",
    "df = pd.read_csv(\"haarlem_today_restaurants.csv\")\n",
    "\n",
    "# Prepare new columns for Google data\n",
    "df[\"Google Name\"] = \"\"\n",
    "df[\"Rating\"] = \"\"\n",
    "df[\"Total Ratings\"] = \"\"\n",
    "df[\"Google Address\"] = \"\"\n",
    "df[\"Place ID\"] = \"\"\n",
    "df[\"Types\"] = \"\"\n",
    "df[\"Price Level\"] = \"\"\n",
    "df[\"Google Website\"] = \"\"\n",
    "df[\"Phone Number\"] = \"\"\n",
    "df[\"Opening Hours\"] = \"\"\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    restaurant_name = str(row[\"Name\"]).strip()\n",
    "    if not restaurant_name or restaurant_name.lower() == \"nan\":\n",
    "        continue\n",
    "\n",
    "    # Step 1: Text Search\n",
    "    query = f\"{restaurant_name} restaurant Haarlem\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"region\": \"nl\",\n",
    "        \"location\": \"52.3874,4.6462\",\n",
    "        \"radius\": 30000,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(SEARCH_URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"results\" in data and len(data[\"results\"]) > 0:\n",
    "        place = data[\"results\"][0]\n",
    "        place_id = place.get(\"place_id\")\n",
    "\n",
    "        df.at[idx, \"Google Name\"] = place.get(\"name\", \"\")\n",
    "        df.at[idx, \"Rating\"] = place.get(\"rating\", \"\")\n",
    "        df.at[idx, \"Total Ratings\"] = place.get(\"user_ratings_total\", \"\")\n",
    "        df.at[idx, \"Google Address\"] = place.get(\"formatted_address\", \"\")\n",
    "        df.at[idx, \"Place ID\"] = place_id\n",
    "        df.at[idx, \"Types\"] = \", \".join(place.get(\"types\", []))\n",
    "\n",
    "        # Step 2: Place Details API\n",
    "        if place_id:\n",
    "            details_params = {\n",
    "                \"place_id\": place_id,\n",
    "                \"fields\": \"price_level,website,formatted_phone_number,opening_hours\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            details_res = requests.get(DETAILS_URL, params=details_params)\n",
    "            details_data = details_res.json().get(\"result\", {})\n",
    "\n",
    "            def map_price_level(level):\n",
    "                mapping = {\n",
    "                    1: \"$\",\n",
    "                    2: \"$$\",\n",
    "                    3: \"$$$\",\n",
    "                    4: \"$$$$\"\n",
    "                }\n",
    "                return mapping.get(level, \"\")\n",
    "            \n",
    "            # Get raw price level from Google Details\n",
    "            raw_price = details_data.get(\"price_level\")\n",
    "            df.at[idx, \"Price Level\"] = map_price_level(raw_price)\n",
    "\n",
    "\n",
    "            df.at[idx, \"Google Website\"] = details_data.get(\"website\", \"\")\n",
    "            df.at[idx, \"Phone Number\"] = details_data.get(\"formatted_phone_number\", \"\")\n",
    "            if details_data.get(\"opening_hours\"):\n",
    "                df.at[idx, \"Opening Hours\"] = \", \".join(details_data[\"opening_hours\"].get(\"weekday_text\", []))\n",
    "\n",
    "        print(f\"✅ {restaurant_name} → {place.get('rating', 'N/A')}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ No results for: {query}\")\n",
    "        print(\"↪️ Response:\", data.get(\"status\"), data.get(\"error_message\"))\n",
    "\n",
    "    time.sleep(2)  # Be kind to the API limits\n",
    "\n",
    "# Save the enriched data\n",
    "df.to_csv(\"haarlem_today_with_full_google_data.csv\", index=False)\n",
    "print(\"✅ Saved to 'haarlem_today_with_full_google_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f7a40-7eb6-4139-b2f1-d526f5273f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from cuisine_keywords import cuisine_keywords  # External Python file containing the keyword dictionary\n",
    "from unidecode import unidecode  # Normalize accented characters\n",
    "\n",
    "# === CONFIG ===\n",
    "API_KEY = \"AIzaSyBPXacrTdwVOZ8vS6nlywLkD3D49Yipz2Q\"\n",
    "SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "CSV_INPUT = \"/mnt/data/haarlem_today_restaurants.csv\"\n",
    "CSV_OUTPUT = \"/mnt/data/haarlem_today_enriched_v2.csv\"\n",
    "\n",
    "# === Load model and cuisine labels ===\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "CUISINES = list(cuisine_keywords.keys())\n",
    "cuisine_embeddings = model.encode(CUISINES, convert_to_tensor=True)\n",
    "\n",
    "# === Read input file ===\n",
    "df = pd.read_csv(CSV_INPUT)\n",
    "\n",
    "# Prepare new columns\n",
    "df[\"Cuisine BERT\"] = \"\"\n",
    "df[\"Cuisine Keywords\"] = \"\"\n",
    "df[\"Keyword Score\"] = \"\"\n",
    "df[\"BERT Score\"] = \"\"\n",
    "df[\"Final Cuisine\"] = \"\"\n",
    "df[\"Certainty\"] = \"\"\n",
    "df[\"Final Score\"] = \"\"\n",
    "df[\"Google Type Cuisine\"] = \"\"\n",
    "df[\"Keyword Hits\"] = \"\"\n",
    "\n",
    "# === Main loop ===\n",
    "for idx, row in df.iterrows():\n",
    "    name = str(row.get(\"Name\", \"\")).strip()\n",
    "    if not name:\n",
    "        continue\n",
    "\n",
    "    query = f\"{name} restaurant Haarlem\"\n",
    "    search_params = {\"query\": query, \"key\": API_KEY}\n",
    "\n",
    "    try:\n",
    "        res = requests.get(SEARCH_URL, params=search_params)\n",
    "        data = res.json()\n",
    "        if not data.get(\"results\"):\n",
    "            continue\n",
    "\n",
    "        place = data[\"results\"][0]\n",
    "        place_id = place[\"place_id\"]\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    details_params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"editorial_summary,reviews,types\",\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        res = requests.get(DETAILS_URL, params=details_params)\n",
    "        details = res.json().get(\"result\", {})\n",
    "\n",
    "        texts = []\n",
    "        if \"editorial_summary\" in details:\n",
    "            texts.append(details[\"editorial_summary\"].get(\"overview\", \"\"))\n",
    "        if \"reviews\" in details:\n",
    "            texts += [r.get(\"text\", \"\") for r in details[\"reviews\"] if \"text\" in r]\n",
    "\n",
    "        combined_text = \" \".join(texts + [name]).lower().strip()\n",
    "        combined_text = unidecode(combined_text)  # Normalize accents\n",
    "\n",
    "        if not combined_text:\n",
    "            continue\n",
    "\n",
    "        # === BERT Matching ===\n",
    "        embedding = model.encode(combined_text, convert_to_tensor=True)\n",
    "        scores = util.cos_sim(embedding, cuisine_embeddings)[0]\n",
    "\n",
    "        # === Keyword Matching with regex ===\n",
    "        match_scores = {}\n",
    "        match_hits = {}\n",
    "\n",
    "        for cuisine, keywords in cuisine_keywords.items():\n",
    "            total_score = 0\n",
    "            hits = []\n",
    "            for kw, weight in keywords.items():\n",
    "                kw_normalized = unidecode(kw.lower())\n",
    "                if re.search(rf\"\\b{re.escape(kw_normalized)}\\b\", combined_text):\n",
    "                    total_score += weight\n",
    "                    hits.append(kw)\n",
    "            if total_score > 0:\n",
    "                match_scores[cuisine] = total_score\n",
    "                match_hits[cuisine] = hits\n",
    "\n",
    "        # === Google Type Cuisine Guess ===\n",
    "        google_types = details.get(\"types\", [])\n",
    "        google_cuisine_guess = None\n",
    "        for t in google_types:\n",
    "            if \"restaurant\" in t and \"_\" in t:\n",
    "                guess = t.split(\"_\")[0].capitalize()\n",
    "                if guess in CUISINES:\n",
    "                    google_cuisine_guess = guess\n",
    "                    break\n",
    "        df.at[idx, \"Google Type Cuisine\"] = google_cuisine_guess if google_cuisine_guess else \"None\"\n",
    "\n",
    "        # === Score Fusion ===\n",
    "        cuisine_scores = {}\n",
    "        for cuisine in CUISINES:\n",
    "            bert_idx = CUISINES.index(cuisine)\n",
    "            bert_score = float(scores[bert_idx])\n",
    "\n",
    "            kw_score = match_scores.get(cuisine, 0)\n",
    "            max_possible = sum(cuisine_keywords[cuisine].values())\n",
    "            kw_norm = kw_score / max_possible if max_possible else 0\n",
    "\n",
    "            fusion_score = (bert_score * 0.6) + (kw_norm * 0.4)\n",
    "            if cuisine == google_cuisine_guess:\n",
    "                fusion_score += 0.1\n",
    "            cuisine_scores[cuisine] = fusion_score\n",
    "\n",
    "        # === Final Cuisine Selection ===\n",
    "        final_cuisine = max(cuisine_scores, key=cuisine_scores.get)\n",
    "        final_score = round(cuisine_scores[final_cuisine], 4)\n",
    "        certainty = \"HIGH\" if final_score > 0.75 else \"MEDIUM\" if final_score > 0.5 else \"LOW\"\n",
    "\n",
    "        # Save debug info\n",
    "        top_bert_idx = int(scores.argmax())\n",
    "        df.at[idx, \"Cuisine BERT\"] = CUISINES[top_bert_idx]\n",
    "        df.at[idx, \"BERT Score\"] = round(float(scores[top_bert_idx]), 10)\n",
    "\n",
    "        if final_cuisine in match_scores:\n",
    "            df.at[idx, \"Cuisine Keywords\"] = f\"{final_cuisine} ({match_scores[final_cuisine]})\"\n",
    "            df.at[idx, \"Keyword Score\"] = match_scores[final_cuisine]\n",
    "            df.at[idx, \"Keyword Hits\"] = \", \".join(match_hits.get(final_cuisine, []))\n",
    "        else:\n",
    "            df.at[idx, \"Cuisine Keywords\"] = \"None\"\n",
    "            df.at[idx, \"Keyword Score\"] = 0\n",
    "            df.at[idx, \"Keyword Hits\"] = \"\"\n",
    "\n",
    "        df.at[idx, \"Final Cuisine\"] = final_cuisine\n",
    "        df.at[idx, \"Certainty\"] = certainty\n",
    "        df.at[idx, \"Final Score\"] = final_score\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    time.sleep(1.5)\n",
    "\n",
    "# Save final DataFrame\n",
    "df.to_csv(CSV_OUTPUT, index=False)\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Cuisine Classification Results\", dataframe=df)\n",
    "\n",
    "# === Save results ===\n",
    "df.to_csv(CSV_OUTPUT, index=False)\n",
    "print(f\"\\n✅ Saved to {CSV_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74720331-bd7b-4b7e-b1ca-b47820f5eab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
